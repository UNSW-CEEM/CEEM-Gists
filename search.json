[
  {
    "objectID": "aemo_data.html",
    "href": "aemo_data.html",
    "title": "AEMO Data Snippets",
    "section": "",
    "text": "This script can be run via the command line to divide a large AEMO data CSV (e.g. from the Monthly Data Archive, such as rebids in BIDPEROFFER) into Parquet partitions. This is advantageous for using packages such as Dask to analyse such data.\nIt assumes that the first row of the table is the header (i.e. columns) for a single data table.\n\n\nWritten using Python 3.11. Uses pathlib and type annotations, so probably need at least Python &gt; 3.5.\n# Python script (executable via CLI) to create parquet partitions\n# for large AEMO data CSVs. Assumes first line is table header and that only one table\n# type is in the file\n#\n# Copyright (C) 2023 Abhijith Prakash\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n\nimport argparse\nimport logging\nfrom pathlib import Path\n\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef arg_parser():\n    description = (\n        \"Chunk large monthly AEMO data table CSVs into parquet partitions. \"\n        + \"Assumes that the table header is in the 2nd row\"\n    )\n    parser = argparse.ArgumentParser(description=description)\n    parser.add_argument(\n        \"-file\", type=str, required=True, help=(\"File to process. Must be CSV\")\n    )\n    parser.add_argument(\n        \"-output_dir\",\n        type=str,\n        required=True,\n        help=(\n            \"Directory to write parquet chunks to. Will be created if it does not exist\"\n        ),\n    )\n    parser.add_argument(\n        \"-chunksize\",\n        type=int,\n        default=10**6,\n        help=(\"Size of each DataFrame chunk (# of lines). Default 10^6\"),\n    )\n    args = parser.parse_args()\n    return args\n\n\ndef get_columns(file_path: Path) -&gt; pd.Index:\n    col_df = pd.read_csv(file_path, header=1, nrows=0)\n    return col_df.columns\n\n\ndef estimate_size_of_lines(file_path: Path, columns=pd.Index) -&gt; float:\n    sample_size = 1000\n    sample = pd.read_csv(file_path, skiprows=2, nrows=sample_size, header=None)\n    sample.columns = columns\n    total_size = sample.memory_usage().sum()\n    size_per_line = total_size / len(sample)\n    return size_per_line\n\n\ndef chunk_file(file_path: Path, output_dir: Path, chunksize: int) -&gt; None:\n    if not file_path.suffix.lower() == \".csv\":\n        logging.error(\"File is not a CSV\")\n        exit()\n    cols = get_columns(file_path)\n    size_per_line = estimate_size_of_lines(file_path, cols)\n    file_size = file_path.stat().st_size\n    file_stem = file_path.stem\n    with pd.read_csv(file_path, chunksize=chunksize, skiprows=2, header=None) as reader:\n        with tqdm(total=file_size, desc=\"Progress estimate based on file size\") as pbar:\n            for i, chunk in enumerate(reader):\n                chunk.columns = cols\n                out_file = Path(file_stem + f\"_chunk{i}.parquet\")\n                chunk.to_parquet(output_dir / out_file)\n                # See here for comparison of pandas DataFrame size vs CSV size:\n                # https://stackoverflow.com/questions/18089667/how-to-estimate-how-much-memory-a-pandas-dataframe-will-need#32970117\n                pbar.update((size_per_line * chunksize) / 2)\n\n\ndef main():\n    logging.basicConfig(format=\"\\n%(levelname)s:%(message)s\", level=logging.INFO)\n    args = arg_parser()\n    f = Path(args.file)\n    output_dir = Path(args.output_dir)\n    if not output_dir.exists():\n        output_dir.mkdir(parents=True)\n    elif len(sorted(output_dir.glob(f.stem + \"*.parquet\"))) &gt; 1:\n        logging.error(\"Pre-existing chunks of this file in output directory. Exiting.\")\n        exit()\n    if not f.exists():\n        logging.error(\"Path does not exist\")\n        exit()\n    if not f.is_file():\n        logging.error(\"Path provided does not point to a file\")\n        exit()\n    chunk_file(f, output_dir, args.chunksize)\n\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "aemo_data.html#dividing-large-aemo-data-csvs-into-parquet-partitions",
    "href": "aemo_data.html#dividing-large-aemo-data-csvs-into-parquet-partitions",
    "title": "AEMO Data Snippets",
    "section": "",
    "text": "This script can be run via the command line to divide a large AEMO data CSV (e.g. from the Monthly Data Archive, such as rebids in BIDPEROFFER) into Parquet partitions. This is advantageous for using packages such as Dask to analyse such data.\nIt assumes that the first row of the table is the header (i.e. columns) for a single data table.\n\n\nWritten using Python 3.11. Uses pathlib and type annotations, so probably need at least Python &gt; 3.5.\n# Python script (executable via CLI) to create parquet partitions\n# for large AEMO data CSVs. Assumes first line is table header and that only one table\n# type is in the file\n#\n# Copyright (C) 2023 Abhijith Prakash\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n\nimport argparse\nimport logging\nfrom pathlib import Path\n\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef arg_parser():\n    description = (\n        \"Chunk large monthly AEMO data table CSVs into parquet partitions. \"\n        + \"Assumes that the table header is in the 2nd row\"\n    )\n    parser = argparse.ArgumentParser(description=description)\n    parser.add_argument(\n        \"-file\", type=str, required=True, help=(\"File to process. Must be CSV\")\n    )\n    parser.add_argument(\n        \"-output_dir\",\n        type=str,\n        required=True,\n        help=(\n            \"Directory to write parquet chunks to. Will be created if it does not exist\"\n        ),\n    )\n    parser.add_argument(\n        \"-chunksize\",\n        type=int,\n        default=10**6,\n        help=(\"Size of each DataFrame chunk (# of lines). Default 10^6\"),\n    )\n    args = parser.parse_args()\n    return args\n\n\ndef get_columns(file_path: Path) -&gt; pd.Index:\n    col_df = pd.read_csv(file_path, header=1, nrows=0)\n    return col_df.columns\n\n\ndef estimate_size_of_lines(file_path: Path, columns=pd.Index) -&gt; float:\n    sample_size = 1000\n    sample = pd.read_csv(file_path, skiprows=2, nrows=sample_size, header=None)\n    sample.columns = columns\n    total_size = sample.memory_usage().sum()\n    size_per_line = total_size / len(sample)\n    return size_per_line\n\n\ndef chunk_file(file_path: Path, output_dir: Path, chunksize: int) -&gt; None:\n    if not file_path.suffix.lower() == \".csv\":\n        logging.error(\"File is not a CSV\")\n        exit()\n    cols = get_columns(file_path)\n    size_per_line = estimate_size_of_lines(file_path, cols)\n    file_size = file_path.stat().st_size\n    file_stem = file_path.stem\n    with pd.read_csv(file_path, chunksize=chunksize, skiprows=2, header=None) as reader:\n        with tqdm(total=file_size, desc=\"Progress estimate based on file size\") as pbar:\n            for i, chunk in enumerate(reader):\n                chunk.columns = cols\n                out_file = Path(file_stem + f\"_chunk{i}.parquet\")\n                chunk.to_parquet(output_dir / out_file)\n                # See here for comparison of pandas DataFrame size vs CSV size:\n                # https://stackoverflow.com/questions/18089667/how-to-estimate-how-much-memory-a-pandas-dataframe-will-need#32970117\n                pbar.update((size_per_line * chunksize) / 2)\n\n\ndef main():\n    logging.basicConfig(format=\"\\n%(levelname)s:%(message)s\", level=logging.INFO)\n    args = arg_parser()\n    f = Path(args.file)\n    output_dir = Path(args.output_dir)\n    if not output_dir.exists():\n        output_dir.mkdir(parents=True)\n    elif len(sorted(output_dir.glob(f.stem + \"*.parquet\"))) &gt; 1:\n        logging.error(\"Pre-existing chunks of this file in output directory. Exiting.\")\n        exit()\n    if not f.exists():\n        logging.error(\"Path does not exist\")\n        exit()\n    if not f.is_file():\n        logging.error(\"Path provided does not point to a file\")\n        exit()\n    chunk_file(f, output_dir, args.chunksize)\n\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CEEM-Gists",
    "section": "",
    "text": "This site hosts code snippets shared by members of the UNSW Collaboration on Energy and Environmental Markets (CEEM)."
  },
  {
    "objectID": "index.html#using-these-snippets",
    "href": "index.html#using-these-snippets",
    "title": "CEEM-Gists",
    "section": "Using these snippets",
    "text": "Using these snippets\nAll source code available on this website and its corresponding repository are licensed under the terms of GPL-3.0-or-later.\nFor the license details, please see the repository license."
  },
  {
    "objectID": "index.html#about-this-site",
    "href": "index.html#about-this-site",
    "title": "CEEM-Gists",
    "section": "About this site",
    "text": "About this site\nThis is a Quarto website."
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Contributing",
    "section": "",
    "text": "Please only share code snippets or scripts that you are happy to be available publicly."
  },
  {
    "objectID": "contributing.html#preparing-your-scriptcode",
    "href": "contributing.html#preparing-your-scriptcode",
    "title": "Contributing",
    "section": "Preparing your script/code",
    "text": "Preparing your script/code\n\nThough the website and repository contains license conditions, we suggest you include this snippet as a preface to any source code:\n&lt;one line to give the program's name and a brief idea of what it does.&gt;\n    Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n[Optional] Format your code using black and isort. This step is optional because these will be automatically run on any pull requests.\nFork this repo or clone and create a new branch\nAdd your source code to the folder that best matches what it does or pertains to.\n\nIf you need to create a new folder/page:\n\nCreate the folder in snippets\nCreate a corresponding .qmd file in the root of the repository\nIn the Quarto YAML config, add the .qmd under the snippets menu"
  }
]